from collections import Counter
import pandas as pd
from cleantext import clean
import emoji
import seaborn as sns

#Load the Dataset
HSDataset = pd.read_csv('Hatespeech_dataset.csv',nrows= 20000)
#Only use certain columns
HS_list = ['toxic','non-toxic']
label_toxic = HSDataset[HSDataset["final_label"].isin(HS_list)]
HSDatasetText = label_toxic["text"]
Size_HSDataset = len(HSDataset['text'])
final_label = label_toxic['final_label']
#####################################PREPROCESSING#########################################################
from cleantext import clean
from emoji.unicode_codes import UNICODE_EMOJI
#remove Stopwords, white spaces, apply Stemming etc...    
CleanList = []
for m in HSDatasetText:
    cleanm = clean(m)
    CleanList.append(cleanm)
i = 0
#Turn Emojis into meaningful text
for e in CleanList:
    CleanList[i] = emoji.demojize(e)
    i+=1


data_corpus = set()
for row in CleanList:
    for word in row.split(" "):
        if word not in data_corpus:
            data_corpus.add(word)

data_corpus=sorted(data_corpus)

#########################INDEX BASED ENCODING#####################################################################
res = len(max(CleanList, key = len).split(" "))
index_based_encoding=[]
for row in CleanList:
    row_encoding = []
    split = row.split(" ")
    for i in range(res):
        if i <= len(split)-1:
            row_encoding.append(data_corpus.index(split[i])+1)
        else:
            row_encoding.append(0)
    index_based_encoding.append(row_encoding)

#######NAIVE BAYES WITH INDEX BASED ENCODING#######################################################################
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns; sns.set()
from sklearn.datasets import make_blobs
 
X = index_based_encoding
y = final_label.values

#Splitting the dataset into training and testing variables
from sklearn.model_selection import train_test_split

#keeping 80% as training data and 20% as testing data. 
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,random_state= 0)
 
from sklearn.naive_bayes import GaussianNB
 
naive_bayes = GaussianNB()
 
#Fitting the data to the classifier
naive_bayes.fit(X_train , y_train)
 
#Predict on test data
y_predicted = naive_bayes.predict(X_test)

from sklearn import metrics
from sklearn.metrics import confusion_matrix
from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score

##########################################Classification Report###################################################
print("INDEX BASED ENCODING - REPORT")
print(metrics.classification_report(y_test, y_predicted))
print("\n")
####################################################################################################################



################ BAG OF WORDS COUNT VECTORIZER NAIVE BAYES#######################################################################
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
cv=CountVectorizer(ngram_range=(1,2),max_features = 30000)
X1= cv.fit_transform(CleanList).toarray()

# Splitting the dataset into the Training set and Test set
from sklearn.model_selection import train_test_split
X1_train, X1_test, y1_train, y1_test = train_test_split(X1, y, test_size = 0.2, random_state = 0)


naive_bayes1 = GaussianNB()
 
#Fitting the data to the classifier
naive_bayes1.fit(X1_train , y1_train)
#########################################Classification Report###################################################
y1_predicted = naive_bayes1.predict(X1_test)
print("BAG OF WORDS - REPORT")
print(metrics.classification_report(y1_test, y1_predicted))
print("\n")
####################################################################################################################


####################TF-IDF NAIVE BAYES#######################################################################

#without smoothing
tf_idf_vec = TfidfVectorizer(use_idf=True, 
                        smooth_idf=False,  
                        ngram_range=(1,1))


tf_idf_data = tf_idf_vec.fit_transform(CleanList).toarray()

X2_train, X2_test, y2_train, y2_test = train_test_split(tf_idf_data, y, test_size = 0.2, random_state = 0)

naive_bayes2 = GaussianNB()

naive_bayes2.fit(X2_train , y2_train)
y2_predicted = naive_bayes2.predict(X2_test)
#########################################Classification Report###################################################
print("TF-IDF WITHOUT SMOOTHING- REPORT")
print(metrics.classification_report(y2_test, y2_predicted))
print("\n")
####################################################################################################################
#with smoothing
tf_idf_vec_smooth = TfidfVectorizer(use_idf=True,  
                        smooth_idf=True,  
                        ngram_range=(1,1))
 
 
tf_idf_data_smooth = tf_idf_vec_smooth.fit_transform(CleanList).toarray()

X3_train, X3_test, y3_train, y3_test = train_test_split(tf_idf_data_smooth, y, test_size = 0.2, random_state = 0)

naive_bayes3 = GaussianNB()

naive_bayes3.fit(X3_train , y3_train)
y3_predicted = naive_bayes3.predict(X3_test)
#########################################Classification Report###################################################
print("TF-IDF WITH SMOOTHING - REPORT")
print(metrics.classification_report(y3_test, y3_predicted))
#################################################################################################################
